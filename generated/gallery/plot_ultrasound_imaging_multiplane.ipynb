{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ultrasound imaging simulation with multi-plane waves\n\nUltrasound imaging is a medical imaging technique that uses sound waves to\ncreate visual representations of internal body structures. It is widely used in\nvarious medical fields, including obstetrics and cardiology, for diagnostic\npurposes. The technology relies on the principle that sound waves can penetrate\nand scatter off tissues, generating echoes that are then used to create detailed\nimages.\n\nIn this notebook, we continue using NDK to simulate the image captured by an\nultrasound transducer, as a follow up to the `plot_ultrasound_imaging_scanline`\nexample.\n\nWe'll follow a similar three step process:\n\n1. transmit pulse\n2. receive echo\n3. reconstruct image\n\nHere, we'll use a phased array transducer to transmit multiple unfocused plane\nwaves at different angles. We'll then receive the echoes and reconstruct the\nimage using the same beamforming and demodulation techniques as the\n`plot_ultrasound_imaging_scanline` example.\n\nFor more details, check out the `plot_ultrasound_imaging_scanline` example\nfirst.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tqdm.notebook\n\nimport neurotechdevkit as ndk\n\n# Imaging modules\nfrom neurotechdevkit.imaging import beamform, demodulate, util\nfrom neurotechdevkit.results import PulsedResult2D, SteadyStateResult2D\nfrom neurotechdevkit.scenarios.built_in import Scenario3\nfrom neurotechdevkit.sources import PhasedArraySource2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scenario parameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SPEED_OF_SOUND_WATER = 1500  # meters per second\n\n# Plane-wave pulse parameters\nTONE_CENTER_FREQUENCY = 0.5e6  # Hz\nTILT_ANGLES_DEG = np.linspace(\n    start=-10, stop=10, endpoint=True, num=5\n)  # Plane-wave pulses\n\n# Phased array transducer parameters\nARRAY_PITCH = 300e-6\nARRAY_ELEMENT_WIDTH = 270e-6\nARRAY_NUM_ELEMENTS = 128\nTRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH = (\n    0.5  # transmit/receive frequency bandwidth as a fraction of center frequency\n)\n\n# Simulation parameters - choosing somewhat arbitrary values\nSAMPLING_POINTS_PER_PERIOD = 24\n\nRANDOM_SEED = 58295  # Use if we need the specific function to be deterministic\nrng = np.random.default_rng(\n    seed=58295\n)  # Use if we only need the notebook to be deterministic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a scenario\n\nUltrasound imaging measures the scattering of sound waves, which occur due to\nchanges in acoustic impedance (speed and density). In normal tissue, these\nchanges occur across different scales. Large tissue boundaries cause specular\nscattering, and microscopic heterogeneities cause diffuse scattering.\n\nTo mimic these properties, we create ultrasound phantoms that are similarly\nheterogeneous across different scales. Their bulk acoustic impedance differs\nfrom the background medium (water), and their within-phantom impedances also\nvary at small scales.\n\nFor the transducer, we choose a phased array that can both steer ultrasonic\nwaves and record independently at multiple elements.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize the scenario layout with the material masks and\nthen visualize the acoustic properties that affect wave scattering and\npropagation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_scenario(tilt_angle: float = 0.0) -> Scenario3:\n    \"\"\"Helper function to initialize scenario with different tilt angles.\"\"\"\n    scenario = Scenario3()\n    scenario.center_frequency = TONE_CENTER_FREQUENCY\n    source_position = [0.01, 0.0]\n    unit_direction = [1.0, 0.0]\n\n    source = PhasedArraySource2D(\n        position=source_position,\n        direction=unit_direction,\n        num_elements=ARRAY_NUM_ELEMENTS,\n        num_points=ARRAY_NUM_ELEMENTS * 4,\n        tilt_angle=tilt_angle,\n        # Unfocused source for plane-wave\n        focal_length=np.inf,\n        pitch=ARRAY_PITCH,\n        element_width=ARRAY_ELEMENT_WIDTH,\n    )\n    scenario.sources = [source]\n    # Place point receivers at the transducer sources\n    scenario.receiver_coords = source.element_positions\n    scenario.make_grid()\n    scenario.compile_problem()\n    return scenario\n\n\nscenario = create_scenario()\nscenario.render_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the relevant material metrics\n\n- vp - speed of sound\n- rho - density\n- alpha - attenuation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfor idx, attribute in enumerate([\"vp\", \"rho\", \"alpha\"]):\n    im = axs[idx].imshow(getattr(scenario.problem.medium, attribute).data)\n    plt.colorbar(im, ax=axs[idx])\n    axs[idx].set_title(attribute)\n    axs[idx].set_xlabel(\"y [a.u.]\")\n    axs[idx].set_ylabel(\"x [a.u.]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Transmit ultrasonic plane waves\nWe run multiple NDK simulations, each sending an ultrasonic plane wave at the\nphantoms at different angles. Each echo provides a low-resolution image of the\ntarget object. By summing the complex-valued images, the low-resolution\nimage's phases cancel out and create a higher-resolution \"compound\" image.\n\nIn this example, we're using the terms pulse/echo/plane-wave somewhat\ninterchangeably, but more specifically pulse=`transmitted ultrasonic wave`,\necho=`the sensor signals received from a given pulse`, and plane-wave=`a\nspecific kind of pulse where the wavefront is a plane/line`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first visualize the plane wave at the default tilt angle (0&deg;).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_steady_state = create_scenario().simulate_steady_state()\nassert isinstance(result_steady_state, SteadyStateResult2D)\nresult_steady_state.render_steady_state_amplitudes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the plane wave sonicates a large area of the\nmedium, including the center phantom and (to a lesser extent) the phantom on\nthe right.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Transmit multi-plane wave\nNow, let's send several plane waves from the transducer and measure the\nreflected/scattered acoustic waves. We can then compound the different plane\nwaves to improve the spatial resolution.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper functions: simulate long enough for ultrasound scattering reflection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calc_simulation_time(scenario: Scenario3):\n    simulation_time = ndk.scenarios._time.select_simulation_time_for_pulsed(\n        grid=scenario.grid,\n        materials=scenario.materials,\n        delay=ndk.scenarios._time.find_largest_delay_in_sources(scenario.sources),\n    )\n    # Run pulse for twice the standard simulation time to allow for reflections\n    return 2 * simulation_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Send several plane waves from the transducer and measure the\nreflected/scattered acoustic waves We can then compound the different plane\nwaves to improve the spatial resolution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results: List[PulsedResult2D] = []\n# keep track of the tx delays used\nelement_delays_list: List[np.ndarray] = []\n\nfor idx, tilt_angle in enumerate(\n    tqdm.notebook.tqdm(TILT_ANGLES_DEG, desc=\"Simulating pulses\", unit=\"pulse\")\n):\n    # Current limitation of NDK: need to re-generate scenario to simulate a new pulse\n    # https://github.com/agencyenterprise/neurotechdevkit/issues/108\n    scenario = create_scenario(tilt_angle=tilt_angle)\n    # Get the element delays (set by the tilt angle)\n    source = scenario.sources[0]\n    assert isinstance(source, ndk.sources.PhasedArraySource)\n    element_delays_list.append(source.element_delays)\n    result = scenario.simulate_pulse(\n        points_per_period=SAMPLING_POINTS_PER_PERIOD,\n        simulation_time=calc_simulation_time(scenario),\n    )\n    results.append(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Receive echo: visualizing received (simulated) signals\n\nIn a real imaging situation, we usually only have access to the RF signals\nmeasured at the ultrasound sensor elements; we don't know what's in the grid\nand can't access the full wavefield.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The receivers at N sensor elements give us data traces of shape:\n`[N, num_fast_time_samples]`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert results[0].traces.data.shape == (\n    ARRAY_NUM_ELEMENTS,\n    len(results[0].traces.time.grid),\n)\ntime_steps = [result.traces.time.step for result in results]\nnp.testing.assert_array_equal(time_steps[0], time_steps)\nfreq_sampling = TONE_CENTER_FREQUENCY * SAMPLING_POINTS_PER_PERIOD\nassert np.allclose(time_steps[0], 1 / freq_sampling)\n\nprint(\"Traces shape:\", results[0].traces.data.shape)\nprint(\"Time grid:\", results[0].traces.time.grid)\nprint(\"Sampling frequency [Hz]: {:.2e}\".format(freq_sampling))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pad RF signals to longest length, in case they are not all the same length\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf_signal_lengths = [len(result.traces.time.grid) for result in results]\nrf_signal_max_len = max(rf_signal_lengths)\nrf_signal_max_len_idx = np.argmax(rf_signal_lengths)\ntime = results[rf_signal_max_len_idx].traces.time.grid\n\n# Shape: `[num_fast_time_samples, num_elements, num_pulses]`\nrf_signals = np.zeros(\n    (rf_signal_max_len, ARRAY_NUM_ELEMENTS, len(results)),\n    dtype=float,\n)\nfor pulse_idx, result in enumerate(results):\n    rf_signals[: rf_signal_lengths[pulse_idx], :, pulse_idx] = result.traces.data.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data at the beginning of each pulse simply contains the transmitted\npulse, so let's remove it from the raw-data visualizations to prevent it from\noverwhelming the reflected image.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "extra_buffer_fraction = 0.05\nsource = scenario.sources[0]\nassert isinstance(source, ndk.sources.PhasedArraySource)\narray_element_positions = source.element_positions\ndistance = np.linalg.norm(array_element_positions.max() - array_element_positions.min())\n\nfor pulse_idx, element_delays in enumerate(element_delays_list):\n    max_time_delay = element_delays.max()\n    last_invalid_time = (1 + extra_buffer_fraction) * (\n        distance / SPEED_OF_SOUND_WATER + max_time_delay\n    )\n    valid_time_mask = time > last_invalid_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_VISUALIZE = 8\nchannel_idxs = rng.integers(ARRAY_NUM_ELEMENTS, size=NUM_VISUALIZE)\npulse_idxs = rng.integers(len(TILT_ANGLES_DEG), size=NUM_VISUALIZE)\n\nrf_signal_visualize = rf_signals[:, channel_idxs, pulse_idxs]\n\n# Plot with some offsets\nCHANNEL_OFFSET = 1000\n_ = plt.plot(\n    time[valid_time_mask] * 1e3,\n    rf_signal_visualize[valid_time_mask]\n    + np.linspace(-0.5, 0.5, num=NUM_VISUALIZE, endpoint=True)\n    * NUM_VISUALIZE\n    * CHANNEL_OFFSET,\n    label=[\n        f\"channel-{channel_idx} echo-{echo_idx}\"\n        for (channel_idx, echo_idx) in zip(channel_idxs, pulse_idxs)\n    ],\n)\n_ = plt.legend()\n_ = plt.title(\"Example radiofrequency signals (RF)\")\n_ = plt.xlabel(\"time [ms]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting across elements (for a single shot) shows the typical\nhyperbolas for each scatterer (object).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pulse_idx = rng.integers(len(TILT_ANGLES_DEG))\n\nfig, ax = plt.subplots()\nax.plot(\n    rf_signals[valid_time_mask, :, pulse_idx]\n    + np.arange(ARRAY_NUM_ELEMENTS) * CHANNEL_OFFSET,\n    time[valid_time_mask] * 1000,\n    color=\"k\",\n)\nax.set_title(f\"echo {pulse_idx}\")\nax.set_xticklabels([])  # We added values for offset, so values don't mean anything\nax.set_ylabel(\"time [ms]\")\nax.set_xlabel(\"channel\")\nax.invert_yaxis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helps to visualize the slight time-offsets of each array\nelement in receiving the echoes.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Reconstruct image\n\nIn multi-plane-wave imaging, each pulse-echo image is constructed separately,\nthen added together.\n\nSimilar to the focused-pulse `109-ultrasound-imaging-scanline.ipynb` notebook,\nthis notebook demodulates the RF signals to I/Q and then beamforms to create\neach pulse-echo frame.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "iq_signals, _ = demodulate.demodulate_rf_to_iq(\n    rf_signals,\n    freq_sampling,\n    freq_carrier=TONE_CENTER_FREQUENCY,\n    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n)\n\nassert iq_signals.shape == rf_signals.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam-form I/Q signals into an image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert len(scenario.sources) == 1\nsource = scenario.sources[0]\nassert isinstance(source, ndk.sources.PhasedArraySource)\npitch = source.pitch\nwidth = source.element_width\nempirical_pitch = np.linalg.norm(\n    np.diff(results[0].shot.receiver_coordinates, axis=0), axis=1\n)\nnp.testing.assert_allclose(empirical_pitch, pitch, rtol=1e-2)\n\n# Generate an image at the scenario grid\n# NOTE: .mesh uses different x/y space\nx_mesh, y_mesh = results[0].shot.grid.space.mesh\n\n# Switch to imaging convention: x for parallel-to-array, z for depth\nimaging_x_mesh = y_mesh + scenario.origin[1]\nimaging_z_mesh = x_mesh + scenario.origin[0]\n\niq_signals_beamformed_list = []\nfor idx, tilt_angle in enumerate(TILT_ANGLES_DEG):\n    iq_signals_beamformed = beamform.beamform_delay_and_sum(\n        iq_signals[:, :, idx],\n        x=imaging_x_mesh,\n        z=imaging_z_mesh,\n        pitch=pitch,\n        tx_delays=element_delays_list[idx],\n        freq_sampling=freq_sampling,\n        freq_carrier=TONE_CENTER_FREQUENCY,\n        f_number=None,\n        width=width,\n        bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n        speed_sound=SPEED_OF_SOUND_WATER,  # water\n    )\n    iq_signals_beamformed_list.append(iq_signals_beamformed)\n\niq_signals_beamformed_compound = np.stack(iq_signals_beamformed_list, axis=-1)\niq_signals_beamformed_compound.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize reconstructed image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_ultrasound_image(x_mesh, z_mesh, iq_signals_bf, db=40):\n    plt.pcolormesh(\n        x_mesh,\n        z_mesh,\n        util.log_compress(iq_signals_bf, db),\n        cmap=\"gray\",\n    )\n    cbar = plt.colorbar(ticks=[0, 1])\n    cbar.ax.set_yticklabels([f\"-{db} dB\", \"0 dB\"])  # horizontal colorbar\n\n    plt.axis(\"equal\")\n    plt.gca().invert_yaxis()  # Invert the y-axis to flip the image vertically\n    plt.title(\"Log-compressed image\")\n    plt.xlabel(\"[m]\")\n    plt.ylabel(\"[m]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DB_VIS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = (imaging_z_mesh > 0.02).all(axis=1)\n\nplot_ultrasound_image(\n    imaging_x_mesh[mask],\n    imaging_z_mesh[mask],\n    iq_signals_beamformed_compound[mask, :, len(TILT_ANGLES_DEG) // 2],\n    db=DB_VIS,\n)\nplt.title(\"Single pulse-echo image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A single plane-wave pulse is unfocused, so the image is blurry.\nOn the other hand, it sonicates a larger area than a focused ultrasound beam\n(a.k.a. \"scan line\"), so it includes both the center circle and the right\ncircle (blurrily).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_ultrasound_image(\n    imaging_x_mesh[mask],\n    imaging_z_mesh[mask],\n    iq_signals_beamformed_compound[mask].sum(axis=-1),\n    db=DB_VIS,\n)\nplt.title(\"Compound image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compounding multiple plane waves improves the image focus.\n\nThe speckles in the image are a normal part of B-mode imaging. Additionally,\nthe phantom on the right experiences a slight \"shadow,\" because not all of the\nplane waves reach it. If we increase the width of the array so that the plane\nwaves are wider, we would get better coverage of the phantom on the right.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}