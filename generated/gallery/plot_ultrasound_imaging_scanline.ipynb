{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUltrasound imaging simulation\n====================================\n\nUltrasound imaging is a medical imaging technique that uses sound waves to\ncreate visual representations of internal body structures. It is widely used\nin various medical fields, including obstetrics and cardiology, for diagnostic\npurposes. The technology relies on the principle that sound waves can\npenetrate and scatter off tissues, generating echoes that are then used to\ncreate detailed images.\n\nUltrasound can be used to measure many different phenomena. Here, we will\ndemonstrate the most common type of ultrasound imaging: B-Mode. B-Mode imaging\nconsists of 3 steps:\n\n1. transmit pulse: a transducer emits high-frequency sound waves into the\n   body. These waves are reflected by large tissue boundaries and diffusely\n   scattered by small irregularities (e.g. cell boundaries).\n2. receive echo: some of these sound waves echo back to the transducer, which\n   records and digitizes them.\n3. reconstruct image: a \"beamforming\" algorithm converts the pressure\n   time-series into an image of the tissue.\n\nJust as we can use NDK to simulate sound waves for focused ultrasound, we can\nuse NDK to simulate the transmitted and received sound waves for ultrasound\nimaging. Ultrasound imaging simulations allow us to easily manipulate\nparameters, such as frequency and focal point, and see how they affect the\nresulting image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tqdm.notebook\n\nimport neurotechdevkit as ndk\n\n# Imaging modules\nfrom neurotechdevkit.imaging import beamform, demodulate, util\nfrom neurotechdevkit.results import PulsedResult2D, SteadyStateResult2D\nfrom neurotechdevkit.scenarios.built_in import Scenario3\nfrom neurotechdevkit.sources import PhasedArraySource2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scenario parameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SPEED_OF_SOUND_WATER = 1500  # meters per second\n\n# Plane-wave pulse parameters\nTONE_CENTER_FREQUENCY = 0.5e6  # Hz\nTILT_ANGLES_DEG = np.linspace(\n    start=-10, stop=10, endpoint=True, num=21\n)  # Plane-wave pulses\n\n# Phased array transducer parameters\nARRAY_PITCH = 300e-6  # meters\nARRAY_ELEMENT_WIDTH = 270e-6  # meters\nARRAY_NUM_ELEMENTS = 128\nTRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH = (\n    0.5  # transmit/receive frequency bandwidth as a fraction of center frequency\n)\n\nRANDOM_SEED = 58295  # Use if we need the specific function to be deterministic\nrng = np.random.default_rng(\n    seed=RANDOM_SEED\n)  # Use if we only need the notebook to be deterministic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the scenario\n\nUltrasound imaging measures the scattering of sound waves, which occur due to\nchanges in acoustic impedance (speed and density). In normal tissue, these\nchanges occur across different scales. Large tissue boundaries cause specular\nscattering, and microscopic heterogeneities cause diffuse scattering.\n\nTo mimic these properties, we create ultrasound phantoms that are similarly\nheterogeneous across different scales. Their bulk acoustic impedance differs\nfrom the background medium (water), and their within-phantom impednances also\nvary at small scales.\n\nFor the transducer, we choose a phased array that can both steer ultrasonic\nwaves and record independently at multiple elements.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize the scenario layout with the material masks and\nthen visualize the acoustic properties that affect wave scattering and\npropagation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_scenario(tilt_angle: float = 0.0) -> Scenario3:\n    \"\"\"Helper function to initialize scenario with different tilt angles.\"\"\"\n    scenario = Scenario3()\n    scenario.center_frequency = TONE_CENTER_FREQUENCY\n    source_position = [0.01, 0.0]\n    unit_direction = [1.0, 0.0]\n    # Focus at the depth of the agar phantom\n    focal_length = np.dot(\n        (scenario.target.center - np.asarray(source_position)), unit_direction\n    )\n\n    source = PhasedArraySource2D(\n        position=source_position,\n        direction=unit_direction,\n        num_elements=ARRAY_NUM_ELEMENTS,\n        num_points=ARRAY_NUM_ELEMENTS * 4,\n        tilt_angle=tilt_angle,\n        focal_length=focal_length,\n        pitch=ARRAY_PITCH,\n        element_width=ARRAY_ELEMENT_WIDTH,\n    )\n    scenario.sources = [source]\n    # Place point receivers at the transducer sources\n    scenario.receiver_coords = source.element_positions\n    scenario.make_grid()\n    scenario.compile_problem()\n    return scenario\n\n\nscenario = create_scenario()\nscenario.render_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the relevant material metrics\n- vp - speed of sound\n- rho - density\n- alpha - attenuation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfor idx, attribute in enumerate([\"vp\", \"rho\", \"alpha\"]):\n    im = axs[idx].imshow(getattr(scenario.problem.medium, attribute).data)\n    plt.colorbar(im, ax=axs[idx])\n    axs[idx].set_title(attribute)\n    axs[idx].set_xlabel(\"y [a.u.]\")\n    axs[idx].set_ylabel(\"x [a.u.]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the bulk contrast between the background medium and\nthe phantoms.\n\nSpeed-of-sound (`vp`) and density (`rho`) determine a material's \"acoustic\nimpedance.\" Changes in acoustic impedance determine how ultrasound waves\nscatter, while attenuation (`alpha`) determines how far ultrasound waves into\na medium.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Transmit pulse\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize beam\n\nTo create a 2-D ultrasound image, a common approach is to emit a focused beam\nat a single horizontal position, effectively creating a 1-D depth image, and\nthen repeat at different horizontal positions. We take this approach here, but\nfor simplicity only simulate a single pulse.\n\nLet's verify that the transducer focuses along a single line. This will be the\nline that receives the most ultrasonic energy and thus will reflect the\nstrongest echoes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_steady_state = create_scenario().simulate_steady_state()\nassert isinstance(result_steady_state, SteadyStateResult2D)\nresult_steady_state.render_steady_state_amplitudes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run pulse for imaging\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scenario = create_scenario()\n\n\ndef calc_simulation_time(scenario: Scenario3):\n    simulation_time = ndk.scenarios._time.select_simulation_time_for_pulsed(\n        grid=scenario.grid,\n        materials=scenario.materials,\n        delay=ndk.scenarios._time.find_largest_delay_in_sources(scenario.sources),\n    )\n    # Run pulse for twice the standard simulation time to allow for reflections\n    return 2 * simulation_time\n\n\nresult = scenario.simulate_pulse(simulation_time=calc_simulation_time(scenario))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Receive echo\nReceiving the echos is already built-in the `simulate_pulse` call above, so\nhere we only need to visualize them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing received (simulated) signals\n\nWhile NDK does provide us the entire wavefield, in a real imaging situation,\nwe usually only have access to the \"radiofrequency signals,\" the raw signals\nmeasured at the ultrasound sensor elements. Let's visualize them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The receivers at N sensor elements give us data traces of shape:\n`[N, num_fast_time_samples]`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_channels = result.shot.num_receivers\nassert num_channels > 0\ntime_arr = result.traces.time.grid\nassert result.traces.data.shape == (num_channels, len(time_arr))\n\nprint(\"Traces shape:\", result.traces.data.shape)\nprint(\"Time grid:\", time_arr)\nfreq_sampling = 1 / result.traces.time.step\nprint(\"Sampling frequency [Hz]: {:.2e}\".format(freq_sampling))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reshape to `[time, channels]` shape\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf_signals = result.traces.data.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_VISUALIZE = 8\nchannel_idxs = rng.integers(num_channels, size=NUM_VISUALIZE)\nchannel_idxs.sort()\n\ntime_mask = (40e-6 < time_arr) & (time_arr < 140e-6)\n\n# Plot with some offsets\n_ = plt.plot(\n    time_arr[time_mask] * 1000,\n    rf_signals[time_mask][:, channel_idxs]\n    + np.linspace(-1000, 1000, num=NUM_VISUALIZE, endpoint=True),\n    label=[f\"channel-{channel_idx}\" for channel_idx in channel_idxs],\n)\n_ = plt.legend()\n_ = plt.title(\"Example radiofrequency signals (RF)\")\n_ = plt.xlabel(\"time [ms]\")\n_ = plt.ylabel(\"amplitude and channel offset [a.u]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transmitted signals consisted of a high-frequency pulse, so\nthe received RF signals are similarly modulated by the same carrier frequency.\nThe signals are strongest about 6-9ms after the pulse transmission.\n\nThe received amplitudes here scale with the transmit pulse, which currently\nhas an arbitrary amplitude. Later down, we rescale the image anyways, so we\nwon't worry about absolute amplitudes here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Reconstruct image\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3a. Demodulate radiofrequency (RF) signals to in-phase/quadrature (I/Q)\n\nNext, we extract the lower-frequency envelope from the radiofrequency signals.\nOne can think of this lower-frequency envelope as matching the spatial\nfrequency of the image.\n\nThere are two slight variants of this initial signal processing step:\nquadrature detection (I/Q) or analytic (Hilbert transform). I/Q is\ntraditionally more common in real-time systems, so we will show it here.\nhttps://starfishmedical.com/blog/pros-cons-two-popular-ultrasound-signal-processing-techniques/\nprovides a nice comparison of the two methods.\n\nI/Q demodulates the center/carrier frequency from the RF signal to extract the\nlower-frequency envelope as a complex signal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "iq_signals, freq_carrier_est = demodulate.demodulate_rf_to_iq(\n    rf_signals,\n    freq_sampling,\n    freq_carrier=TONE_CENTER_FREQUENCY,\n    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n)\n\nassert iq_signals.shape == rf_signals.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the lower-frequency I/Q signals over the RF signal\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "channel_idx = rng.integers(\n    num_channels\n)  # Pick one of the earlier channels? random choice?\n# zoom in on time-window\n\nfig, ax = plt.subplots()\nax.plot(\n    time_arr[time_mask] * 1000,\n    rf_signals[time_mask, channel_idx],\n    color=\"k\",\n    alpha=0.3,\n    label=\"RF\",\n)\nax.plot(\n    time_arr[time_mask] * 1000,\n    iq_signals[time_mask, channel_idx].real,\n    label=\"in-phase (I)\",\n)\nax.plot(\n    time_arr[time_mask] * 1000,\n    iq_signals[time_mask, channel_idx].imag,\n    label=\"quadrature (Q)\",\n)\nax.legend()\nax.set_title(f\"RF & I/Q signal. Channel: {channel_idx}\")\nax.set_xlabel(\"time [ms]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b. Beamform image\nLet's use a common beamforming algorithm, `delay-and-sum`, to reconstruct an\nultrasound image.\n\nThe intuition for `delay-and-sum` is that: if an object is at distance $d$\nfrom a given transducer with speed-of-sound $c$, we should receive its\nscattered signal at time $\\frac{d}{c}$ after it receives the ultrasound pulse.\nThis is a similar idea to shouting in a cave, then waiting to hear the\necho-time to determine how far away the wall is. In 2 or 3 dimensions,\nhowever, there are multiple points at the same distance that could be\ncontributing to the echo signal. Delay-and-sum implicitly disassociates these\nconfounded points by summing over the different receiver elements.\n\nNote: beamforming can also work on the raw radiofrequency (RF) signals.\nHowever, the RF signals are higher-frequency and thus require additional\ncomputational power, so diagnostic B-mode scanners usually demodulate RF\nsignals to I/Q first (as we did above).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam-form I/Q signals into an image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert len(scenario.sources) == 1\nsource = scenario.sources[0]\nassert isinstance(source, ndk.sources.PhasedArraySource)\npitch = source.pitch\nwidth = source.element_width\nempirical_pitch = np.linalg.norm(\n    np.diff(result.shot.receiver_coordinates, axis=0), axis=1\n)\nnp.testing.assert_allclose(empirical_pitch, pitch, rtol=1e-2)\n\n# Generate an image at the scenario grid\n# NOTE: .mesh uses different x/y space\nx_mesh, y_mesh = result.shot.grid.space.mesh\n\n# Switch to imaging convention: x for parallel-to-array, z for depth\nimaging_x_mesh = y_mesh + scenario.origin[1]\nimaging_z_mesh = x_mesh + scenario.origin[0]\n\niq_signals_beamformed = beamform.beamform_delay_and_sum(\n    iq_signals,\n    x=imaging_x_mesh,\n    z=imaging_z_mesh,\n    pitch=pitch,\n    tx_delays=source.element_delays,\n    freq_sampling=freq_sampling,\n    freq_carrier=TONE_CENTER_FREQUENCY,\n    f_number=None,\n    width=width,\n    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n    speed_sound=SPEED_OF_SOUND_WATER,  # water\n)\niq_signals_beamformed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize reconstructed image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_ultrasound_image(x_mesh, z_mesh, iq_signals_bf, db=40):\n    plt.pcolormesh(\n        x_mesh,\n        z_mesh,\n        util.log_compress(iq_signals_bf, db),\n        cmap=\"gray\",\n    )\n    cbar = plt.colorbar(ticks=[0, 1])\n    cbar.ax.set_yticklabels([f\"-{db} dB\", \"0 dB\"])  # horizontal colorbar\n\n    plt.axis(\"equal\")\n    plt.gca().invert_yaxis()  # Invert the y-axis to flip the image vertically\n    plt.title(\"Log-compressed image\")\n    plt.xlabel(\"[m]\")\n    plt.ylabel(\"[m]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ignore the transducer elements\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = (imaging_z_mesh > 0.02).all(axis=1)\nplot_ultrasound_image(\n    imaging_x_mesh[mask],\n    imaging_z_mesh[mask],\n    iq_signals_beamformed[mask],\n    db=30,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above shows the reconstruction of a single ultrasound scan line,\nfocusing on $x = 0$ (e.g., see the steady-state amplitude, which shows where\nthe pulse focuses). Because the phantom on the right is out of the focal\npoint, it does not show up clearly in the image. Sweeping across different\nscan lines would allow reconstruction of a full 2D (or 3D) image.\n\nThe speckles in the image are a normal part of B-mode imaging. Further signal\nprocessing methods, such as harmonic imaging or compound-plane-wave imaging,\ncan improve spatial resolution.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct full image by sweeping scan-lines\n\nNext, let's build on the previous example by constructing a wider\nfield-of-view by sweeping different scan lines. We can use the phased-array to\ntilt the scan line radially and image different parts of the image space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "NUM_TILTS = 5\nTILT_ANGLES_DEG = np.linspace(-40, 40, NUM_TILTS, endpoint=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compare the beam-focus of 2 different tilt angles\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scenario = create_scenario(tilt_angle=TILT_ANGLES_DEG[0])\nassert len(scenario.sources) == 1\nresult_steady_state = scenario.simulate_steady_state()\nassert isinstance(result_steady_state, SteadyStateResult2D)\nresult_steady_state.render_steady_state_amplitudes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scenario = create_scenario(tilt_angle=TILT_ANGLES_DEG[-1])\nassert len(scenario.sources) == 1\nresult_steady_state = scenario.simulate_steady_state()\nassert isinstance(result_steady_state, SteadyStateResult2D)\nresult_steady_state.render_steady_state_amplitudes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The steady-state figures show that the tilted ultrasound beams\nhit different parts of the image space.\n\nNow, let's pulse at different scan lines and sum them together to reconstruct\nthe image.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Send several plane waves from the transducer and measure the\nreflected/scattered acoustic waves We can then compound the different plane\nwaves to improve the spatial resolution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results: List[PulsedResult2D] = []\n# keep track of the tx delays used\nelement_delays_list = []\n\nfor idx, tilt_angle in enumerate(\n    tqdm.notebook.tqdm(TILT_ANGLES_DEG, desc=\"Simulating pulses\", unit=\"pulse\")\n):\n    # Current limitation of NDK: need to re-generate scenario to simulate a new pulse\n    # https://github.com/agencyenterprise/neurotechdevkit/issues/108\n    # Set the tilt angle, which will automatically re-calculate the element delays\n    scenario = create_scenario(tilt_angle=tilt_angle)\n    assert len(scenario.sources) == 1\n    source = scenario.sources[0]\n    assert isinstance(source, ndk.sources.PhasedArraySource)\n    element_delays_list.append(source.element_delays)\n    result = scenario.simulate_pulse(simulation_time=calc_simulation_time(scenario))\n    results.append(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's follow the same steps as above for reconstructing each scan-line image.\n\nStack the different scan-lines (sometimes called the \"slow-time\" dimension).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pad RF signals to longest length, in case they are not all the same length\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf_signal_lengths = [len(result.traces.time.grid) for result in results]\nrf_signal_max_len = max(rf_signal_lengths)\nrf_signal_max_len_idx = np.argmax(rf_signal_lengths)\ntime = results[rf_signal_max_len_idx].traces.time.grid\n\n# Shape: `[num_fast_time_samples, num_elements, num_pulses]`\nrf_signals = np.zeros(\n    (rf_signal_max_len, ARRAY_NUM_ELEMENTS, len(results)),\n    dtype=float,\n)\nfor pulse_idx, result in enumerate(results):\n    rf_signals[: rf_signal_lengths[pulse_idx], :, pulse_idx] = result.traces.data.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Demodualte the RF signals to I/Q.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "freq_sampling = 1 / result.traces.time.step\niq_signals, _ = demodulate.demodulate_rf_to_iq(\n    rf_signals,\n    freq_sampling,\n    freq_carrier=TONE_CENTER_FREQUENCY,\n    bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n)\n\nassert iq_signals.shape == rf_signals.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beamform I/Q signals into an image\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam-form I/Q signals into an image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert len(scenario.sources) == 1\nsource = scenario.sources[0]\nassert isinstance(source, ndk.sources.PhasedArraySource)\npitch = source.pitch\nwidth = source.element_width\nempirical_pitch = np.linalg.norm(\n    np.diff(results[0].shot.receiver_coordinates, axis=0), axis=1\n)\nnp.testing.assert_allclose(empirical_pitch, pitch, rtol=1e-2)\n\n# Generate an image at the scenario grid\n# NOTE: .mesh uses different x/y space\nx_mesh, y_mesh = results[0].shot.grid.space.mesh\n\n# Switch to imaging convention: x for parallel-to-array, z for depth\nimaging_x_mesh = y_mesh + scenario.origin[1]\nimaging_z_mesh = x_mesh + scenario.origin[0]\n\niq_signals_beamformed_list = []\nfor idx, tilt_angle in enumerate(TILT_ANGLES_DEG):\n    iq_signals_beamformed = beamform.beamform_delay_and_sum(\n        iq_signals[:, :, idx],\n        x=imaging_x_mesh,\n        z=imaging_z_mesh,\n        pitch=pitch,\n        tx_delays=element_delays_list[idx],\n        freq_sampling=freq_sampling,\n        freq_carrier=TONE_CENTER_FREQUENCY,\n        f_number=None,\n        width=width,\n        bandwidth=TRANSDUCER_6DB_PULSE_ECHO_FRACTIONAL_BANDWIDTH,\n        speed_sound=SPEED_OF_SOUND_WATER,  # water\n    )\n    iq_signals_beamformed_list.append(iq_signals_beamformed)\n\niq_signals_beamformed_compound = np.stack(iq_signals_beamformed_list, axis=-1)\niq_signals_beamformed_compound.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask = (imaging_z_mesh > 0.02).all(axis=1)\nplot_ultrasound_image(\n    imaging_x_mesh[mask],\n    imaging_z_mesh[mask],\n    iq_signals_beamformed_compound[mask].sum(axis=-1),\n    db=30,\n)\nplt.title(\"Multi-scan-line image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The multi-scan-line image includes both phantoms. Notice,\nthough, that the image of the center phantom is more intense where the scan\nline was focused. This resolution can be improved by sweeping scan lines that\nare closer together. Thus, the resolution is tightly coupled with the number\nof scan lines, analogous to raster-printing a photo.\n\nIn the next notebook, `plot_ultrasound_imaging_multiplane`, we will discuss a\nmethod to improve resolution with a fewer number of pulse-echos by using\nunfocused beams.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}